# RM-Distiller: Exploiting Generative LLM for Reward Model Distillation

This is the official repository for paper _RM-Distiller: Exploiting Generative LLM for Reward Model Distillation_.

In this paper, we introduce RM-Distiller, a framework designed to distill discriminative reward models (RMs) from generative LLMs.

---

## ğŸ“‚ Repository Structure

```text
.
â”œâ”€â”€ refine/            # Contrastive Refinement inluding response diagnosis and minimal editing
â”œâ”€â”€ score/             # Preference strength annotation and Self-Calibrated Scoring
â”œâ”€â”€ RewardTrainer/           # RM training and evaluation
â”‚   â”œâ”€â”€ configs/             # Generative RM evaluation configuration files
â”‚   â”œâ”€â”€ eval/                # Evaluation scripts and configuration files
â”‚   â”œâ”€â”€ scripts/             # Entry scripts for training and evaluation
â”‚   â”‚   â”œâ”€â”€ train_rm.sh              # BT Classfier training
â”‚   â”‚   â”œâ”€â”€ train_distilrm.sh        # RM-Distiller training
â”‚   â”‚   â”œâ”€â”€ eval_rm.sh               # Discriminative RM evaluation
â”‚   â”‚   â”œâ”€â”€ eval_judge.sh            # Generative RM evaluation
â”‚   â””â”€â”€ train/               # Core training logic and model implementations
â”œâ”€â”€ .gitignore         # Git ignore rules
â”œâ”€â”€ README.md          # Project documentation
â””â”€â”€ requirements.txt  # Python dependencies
```

## âš¡ï¸ Usage

### Preparation

Please refer to the following commands to prepare your environment.

```shell
conda create -n rm-distiller python=3.12
pip install -r requirements.txt
```

### Contrastive Refinement

To synthesize highly contrastive preference pairs via teacher-guided minimal refinement, run the refinement pipeline in the `refine/` directory.

```bash
python refine/refine_response_vllm.py
```

### Self-Calibrated Scoring

To assign calibrated preference scores and obtain preference strength margins, run the scoring scripts in the `score/` directory.

```bash
python score/cali_score_vllm.py
```

### RM Training

To train the RM with Margin-Aware Regression and Generative Regularization, use the training scripts provided in `RewardTrainer/scripts/`.

```bash
cd RewardTrainer
bash scripts/train_distilrm.sh
```
