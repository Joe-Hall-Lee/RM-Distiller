#!/usr/bin/env python
# -*- coding: utf-8 -*-
import json
import os
import uuid
import re
from tqdm import tqdm
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# --- é…ç½® ---
DATA_PATH = "data/skywork_10k.jsonl"   # è¾“å…¥æ–‡ä»¶è·¯å¾„
OUTPUT_FILE = "RewardTrainer/data/train/skywork_10k_orig_qwen.json" # è¾“å‡ºæ–‡ä»¶è·¯å¾„
TEACHER_MODEL_PATH = "models/Qwen3-14B" # è£åˆ¤æ¨¡å‹è·¯å¾„

# è¯„ä¼° Prompt æ¨¡æ¿
EVAL_PROMPT_TEMPLATE = """[Question]
{question}

[The Start of Assistant 1's Answer]
{answer_1}

[The End of Assistant 1's Answer]

[The Start of Assistant 2's Answer]
{answer_2}

[The End of Assistant 2's Answer]

[System]
We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.
Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.
Please output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by a space. Avoid any potential bias and ensure that the order in which the responses were presented does not affect your judgment. Do NOT provide any explanation.

### Response:"""


def load_data(file_path):
    """åŠ è½½æ•°æ®ï¼Œè¿”å›åˆ—è¡¨"""
    data_list = []
    if not os.path.exists(file_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
        return []

    print(f"æ­£åœ¨è¯»å– {file_path} ...")
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip():
                continue
            try:
                item = json.loads(line.strip())
                # ç®€å•æ ¡éªŒå¿…è¦å­—æ®µ
                if all(k in item for k in ['question', 'chosen', 'rejected']):
                    data_list.append(item)
            except json.JSONDecodeError:
                continue
    
    print(f"æˆåŠŸåŠ è½½ {len(data_list)} æ¡æ•°æ®")
    return data_list


def format_eval_prompt(question, answer_1, answer_2):
    """æ ¼å¼åŒ– Prompt"""
    return EVAL_PROMPT_TEMPLATE.format(question=question, answer_1=answer_1, answer_2=answer_2)


def extract_scores(response_text):
    """
    ä»æ¨¡å‹å›å¤ä¸­è§£æåˆ†æ•°ã€‚
    å‡è®¾å›å¤æ ¼å¼ç±»ä¼¼ "8 2" æˆ– "9.5 6.0"
    """
    if not response_text:
        return 0.0, 0.0
    
    # æ­£åˆ™åŒ¹é…æµ®ç‚¹æ•°æˆ–æ•´æ•°
    matches = re.findall(r"[-+]?\d*\.\d+|\d+", response_text)
    
    if len(matches) >= 2:
        try:
            # å–å‰ä¸¤ä¸ªæ•°å­—ï¼Œåˆ†åˆ«å¯¹åº” Assistant 1 (Chosen) å’Œ Assistant 2 (Rejected)
            s1 = float(matches[0])
            s2 = float(matches[1])
            return s1, s2
        except ValueError:
            return 0.0, 0.0
    return 0.0, 0.0


def evaluate_dataset(teacher_model_path, data_list):
    """ä½¿ç”¨ vLLM æ‰¹é‡è¯„ä¼°å¹¶ç”Ÿæˆæœ€ç»ˆ JSON"""
    
    # 1. åˆå§‹åŒ–æ¨¡å‹
    print(f"æ­£åœ¨åŠ è½½æ•™å¸ˆæ¨¡å‹: {teacher_model_path} ...")
    llm = LLM(model=teacher_model_path, 
              trust_remote_code=True, 
              max_model_len=8192,
              gpu_memory_utilization=0.9,
              tensor_parallel_size=1)

    tokenizer = AutoTokenizer.from_pretrained(teacher_model_path, trust_remote_code=True)

    # 2. é‡‡æ ·å‚æ•° (Temperature=0 ä¿è¯ç¡®å®šæ€§)
    sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=20)

    # 3. æ„å»º Prompts
    prompts_payload = []
    
    # ç”¨äºä¿ç•™åŸå§‹æ•°æ®å¼•ç”¨çš„åˆ—è¡¨ï¼Œç¡®ä¿é¡ºåºä¸€è‡´
    valid_items = [] 

    print("æ­£åœ¨æ„å»º Prompts ...")
    for item in tqdm(data_list):
        question = item['question']
        answer_1 = item['chosen']   # Assistant 1 å¯¹åº” Chosen
        answer_2 = item['rejected'][0] # Assistant 2 å¯¹åº” Rejected

        user_content = format_eval_prompt(question, answer_1, answer_2)
        
        messages = [
            {"role": "system", "content": "You are a helpful and precise assistant for checking the quality of the answer."},
            {"role": "user", "content": user_content}
        ]

        # å¤„ç† Chat Template
        try:
            formatted_prompt = tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True, enable_thinking=False
            )
        except TypeError:
            formatted_prompt = tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
        prompts_payload.append(formatted_prompt)
        valid_items.append(item)

    # 4. æ‰¹é‡æ¨ç† (vLLM æ ¸å¿ƒæ­¥éª¤)
    print(f"å¼€å§‹æ‰¹é‡æ¨ç† {len(prompts_payload)} æ¡æ•°æ® ...")
    outputs = llm.generate(prompts_payload, sampling_params)

    # 5. æ•´ç†ç»“æœ
    final_results = []
    
    print("æ­£åœ¨è§£æåˆ†æ•°å¹¶æ•´ç†ç»“æœ ...")
    # zip ç¡®ä¿åŸå§‹æ•°æ®å’Œæ¨¡å‹è¾“å‡ºä¸€ä¸€å¯¹åº” 
    for original_item, output in zip(valid_items, outputs):
        generated_text = output.outputs[0].text
        
        # è§£æåˆ†æ•°
        score1, score2 = extract_scores(generated_text)
        
        # æ„é€ ç›®æ ‡æ ¼å¼
        result_entry = {
            "id": str(uuid.uuid4()),  # ç”Ÿæˆ UUID
            "prompt": original_item['question'],
            "chosen": original_item['chosen'],
            "rejected": original_item['rejected'][0],
            "chosen_score": score1,   # å¯¹åº” Prompt ä¸­çš„ Assistant 1
            "rejected_score": score2  # å¯¹åº” Prompt ä¸­çš„ Assistant 2
        }
        final_results.append(result_entry)

    # 6. ä¿å­˜ä¸ºå•ä¸ªå¤§ JSON æ–‡ä»¶
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    print(f"æ­£åœ¨ä¿å­˜ {len(final_results)} æ¡ç»“æœåˆ° {OUTPUT_FILE} ...")
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, ensure_ascii=False, indent=4)

    print("ğŸ‰ è¯„ä¼°å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜ã€‚")


def main():
    if not os.path.exists(DATA_PATH):
        print(f"è¯·ç¡®è®¤æ•°æ®æ–‡ä»¶è·¯å¾„æ­£ç¡®: {DATA_PATH}")
        return

    data = load_data(DATA_PATH)
    if not data:
        return

    evaluate_dataset(TEACHER_MODEL_PATH, data)


if __name__ == "__main__":
    main()