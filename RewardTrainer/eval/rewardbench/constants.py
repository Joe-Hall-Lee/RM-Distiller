# Copyright 2023 AllenAI. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# reference for length bias categories
LENGTH_CATEGORIES = {
    "alpacaeval-easy": "True",
    "alpacaeval-hard": "True",
    "alpacaeval-length": "Neutral",
    "donotanswer": "False",
    "hep-cpp": "Neutral",
    "hep-go": "Neutral",
    "hep-java": "Neutral",
    "hep-js": "Neutral",
    "hep-python": "Neutral",
    "hep-rust": "Neutral",
    "llmbar-adver-GPTInst": "False",
    "llmbar-adver-GPTOut": "Neutral",
    "llmbar-adver-manual": "False",
    "llmbar-adver-neighbor": "False",
    "llmbar-natural": "Neutral",
    "math-prm": "Neutral",
    "mt-bench-easy": "False",
    "mt-bench-hard": "False",
    "mt-bench-med": "Neutral",
    "refusals-dangerous": "False",
    "refusals-offensive": "False",
    "xstest-should-refuse": "False",
    "xstest-should-respond": "True",
}

EXAMPLE_COUNTS = {
    "alpacaeval-easy": 100,
    "alpacaeval-length": 95,
    "alpacaeval-hard": 95,
    "mt-bench-easy": 28,
    "mt-bench-med": 40,
    "mt-bench-hard": 37,
    "math-prm": 984,  # actual length 447, upweighting to be equal to code
    "refusals-dangerous": 100,
    "refusals-offensive": 100,
    "llmbar-natural": 100,
    "llmbar-adver-neighbor": 134,
    "llmbar-adver-GPTInst": 92,
    "llmbar-adver-GPTOut": 47,
    "llmbar-adver-manual": 46,
    "xstest-should-refuse": 154,
    "xstest-should-respond": 250,
    "donotanswer": 136,
    "hep-cpp": 164,
    "hep-go": 164,
    "hep-java": 164,
    "hep-js": 164,
    "hep-python": 164,
    "hep-rust": 164,
    "mmlu-pro-biology": 11,
    "mmlu-pro-business": 11,
    "mmlu-pro-chemistry": 11,
    "mmlu-pro-computer science": 11,
    "mmlu-pro-economics": 11,
    "mmlu-pro-engineering": 11,
    "mmlu-pro-health": 11,
    "mmlu-pro-history": 11,
    "mmlu-pro-law": 11,
    "mmlu-pro-math": 11,
    "mmlu-pro-other": 11,
    "mmlu-pro-philosophy": 11,
    "mmlu-pro-physics": 11,
    "mmlu-pro-psychology": 11,
    "livebench-reasoning": 98,
    "livebench-math": 56,
    "livecodebench": 42,
    # Harmlessness 部分
    "Child Sexual Exploitation": 537,
    "Hate": 543,
    "Indiscriminate Weapons": 519,
    "Intellectual Property": 510,
    "Multi": 502,
    "Non-Violent Crimes": 1040,
    "Privacy": 317,
    "Sex-Related Crimes": 739,
    "Sexual Content": 977,
    "Specialized Advice": 561,
    "Suicide & Self-Harm": 360,
    "Violent Crimes": 459,

    # Helpfulness 部分
    "Brainstorming": 949,
    "Chat": 713,
    "Classification": 164,
    "Closed QA": 920,
    "Code": 1224,
    "Generation": 1109,
    "Open QA": 1542,
    "Reasoning": 1235,
    "Rewrite": 458,
    "Role Playing": 372,
    "Summarization": 530,
    "Translation": 851,
    # RM-Bench 部分
    "Chat_Easy": 387,
    "Chat_Hard": 387,
    "Chat_Normal": 387,
    "Code_Easy": 684,
    "Code_Hard": 684,
    "Code_Normal": 684,
    "Math_Easy": 1587,
    "Math_Hard": 1587,
    "Math_Normal": 1587,
    "Safety_Easy": 1323,
    "Safety_Hard": 1323,
    "Safety_Normal": 1323,
    "ifbench": 444
}

SUBSET_MAPPING = {
    "Chat (RewardBench)": [
        "alpacaeval-easy",
        "alpacaeval-length",
        "alpacaeval-hard",
        "mt-bench-easy",
        "mt-bench-med",
    ],
    "Chat Hard": [
        "mt-bench-hard",
        "llmbar-natural",
        "llmbar-adver-neighbor",
        "llmbar-adver-GPTInst",
        "llmbar-adver-GPTOut",
        "llmbar-adver-manual",
    ],
    "Safety (RewardBench)": [
        "refusals-dangerous",
        "refusals-offensive",
        "xstest-should-refuse",
        "xstest-should-respond",
        "donotanswer",
    ],
    "Reasoning (RewardBench)": [
        "math-prm",
        "hep-cpp",
        "hep-go",
        "hep-java",
        "hep-js",
        "hep-python",
        "hep-rust",
    ],
    "Knowledge": ["mmlu-pro-biology",
                 "mmlu-pro-business",
                 "mmlu-pro-chemistry",
                 "mmlu-pro-computer science",
                 "mmlu-pro-economics",
                 "mmlu-pro-engineering",
                 "mmlu-pro-health",
                 "mmlu-pro-history",
                 "mmlu-pro-law",
                 "mmlu-pro-math",
                 "mmlu-pro-other",
                 "mmlu-pro-philosophy",
                 "mmlu-pro-physics",
                 "mmlu-pro-psychology"
                 ],
    "Reasoning": ["livebench-reasoning"],
    "Math (JudgeBench)": ["livebench-math"],
    "Coding": ["livecodebench"],
    "Harmlessness": [
        "Violent Crimes",
        "Non-Violent Crimes",
        "Sex-Related Crimes",
        "Child Sexual Exploitation",
        "Specialized Advice",
        "Privacy",
        "Intellectual Property",
        "Indiscriminate Weapons",
        "Hate",
        "Suicide & Self-Harm",
        "Sexual Content",
        "Multi",
    ],
    "Helpfulness": [
        "Brainstorming",
        "Chat",
        "Classification",
        "Closed QA",
        "Code",
        "Generation",
        "Open QA",
        "Reasoning",
        "Rewrite",
        "Role Playing",
        "Summarization",
        "Translation",
    ],
    "Chat (RM-Bench)": [
        "Chat_Easy",
        "Chat_Normal",
        "Chat_Hard",
    ],
    "Math": [
        "Math_Easy",
        "Math_Normal",
        "Math_Hard",
    ],
    "Code": [
        "Code_Easy",
        "Code_Normal",
        "Code_Hard",
    ],
    "Safety": [
        "Safety_Easy",
        "Safety_Normal",
        "Safety_Hard",
    ],
    "IFBench": ["ifbench"]
}
